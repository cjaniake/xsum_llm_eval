# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.16.7
#   kernelspec:
#     display_name: hf
#     language: python
#     name: python3
# ---

# %% [markdown]
# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cjaniake/xsum_llm_eval/blob/main/docs/getting_started.ipynb)
#
# # Evaluating LLM applications

# %%
import litellm
import pandas as pd

from datasets import load_dataset
from deepeval import evaluate
from deepeval.metrics import SummarizationMetric
from deepeval.test_case import LLMTestCase
from litellm.types.completion import ChatCompletionMessageParam
from typing import List

from IPython.display import display, Markdown

# %% [markdown]
# ### Load the XSum dataset from HuggingFace Datasets
#
# The library keeps a local copy and does not re-download it every time

# %%
xsum_dataset = load_dataset("EdinburghNLP/xsum")

# %%
xsum_dataset["validation"].to_pandas().head(3)

# %% [markdown]
# ### Draw a sample from the dataset
#
# Let's take 4 random examples and analyse the summaries generated by GTP-4o-mini

# %%
RANDOM_STATE = 123
sample_df = xsum_dataset["validation"].to_pandas().sample(4, random_state=RANDOM_STATE)
sample_df

# %%
system_prompt = {
    "content": "Your task is to summarize the provided text. "
    + "Explain in one sentence what the news article is about. "
    + "Your response should be direct, don't mention the article, just give me the summary. "
    + "You should cover the main facts present in the text and avoid adding facts. "
    + "Be brief and straight to the point",
    "role": "system",
}


def summarization_prompt(document: str) -> ChatCompletionMessageParam:
    return [system_prompt, {"content": document, "role": "user"}]


def run_summarization(summarization_model, input_document):
    llm_response = litellm.completion(
        model=summarization_model,
        messages=summarization_prompt(input_document),
        max_tokens=200,
    )
    response = llm_response["choices"][0]["message"]["content"]
    return response


def summarize_samples(sample_df, summarization_model):
    return [
        run_summarization(summarization_model, r["document"])
        for _, r in sample_df.iterrows()
    ]


# %%
gpt_4o_mini_summaries = summarize_samples(sample_df, "gpt-4o-mini")

# %%
for ground_truth_summary, generated_summary in zip(
    sample_df.summary, gpt_4o_mini_summaries
):
    display(Markdown(f"**Ground Truth:** {ground_truth_summary}"))
    display(Markdown(f"**Generated Summary:** {generated_summary}"))
    display(Markdown("---"))


# %% [markdown]
# ### Explore the summarization metric

# %%
def evaluate_samples(sample_df, summaries, evaluation_model):
    eval_scores = []
    for t, generated_summary in zip(sample_df.iterrows(), summaries):
        sample_data = t[1]
        input_document = sample_data["document"]

        test_case = LLMTestCase(input=input_document, actual_output=generated_summary)
        metric = SummarizationMetric(
            threshold=0.5, model=evaluation_model, verbose_mode=False, async_mode=False
        )

        metric.measure(test_case, _show_indicator=False)
        eval_scores.append(metric.score_breakdown | {"Score": metric.score})

        display(Markdown(f"#### Input Text\n> {input_document}"))
        display(Markdown(f"#### Summary\n{generated_summary}"))
        display(Markdown(f"#### Score: **{metric.score}**"))
        display(Markdown(metric.reason))
        display(
            Markdown(
                "\n".join(
                    f"- {k.lower()} score: {v:.2f}"
                    for k, v in metric.score_breakdown.items()
                )
            )
        )
        display(Markdown("---"))
    return pd.DataFrame(eval_scores)


# %% [markdown]
# ## Evaluate the generated summaries
#
# Run the evaluation on a small sample

# %%
generated_summary_eval_scores = evaluate_samples(
    sample_df, gpt_4o_mini_summaries, "gpt-4o"
)

# %% [markdown]
# ## Evaluate the ground truth
#
# Run the evaluation on the human annotated summary

# %%
ground_truth_eval_scores = evaluate_samples(sample_df, sample_df.summary, "gpt-4o")

# %%
import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(ncols=2, figsize=(20, 6), sharey=True)
sns.boxplot(ground_truth_eval_scores, ax=ax[0])
sns.boxplot(generated_summary_eval_scores, ax=ax[1])
ax[0].set_title("Ground Truth evaluation")
ax[1].set_title("Generated Summaries evaluation")

# %%
